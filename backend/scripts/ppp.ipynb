{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87df35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017/cs5600\")\n",
    "PRODUCTS_COLL = os.getenv(\"PRODUCTS_COLL\", \"products\")\n",
    "SIG_COLL = os.getenv(\"SIG_COLL\", \"productsignatures\")\n",
    "\n",
    "K_VALUES = [2, 3, 5, 7, 10]\n",
    "HASH_VALUES = [10, 20, 50, 100, 150]\n",
    "NUM_BANDS_VALUES = [4, 5, 10, 15, 20, 25]\n",
    "MAX_HASHES = max(HASH_VALUES)\n",
    "NUM_WORKERS = max(1, min(cpu_count() - 1, 12))\n",
    "TUNING_SAMPLE = None\n",
    "TOP_K = 10\n",
    "BULK_BATCH = 500\n",
    "# ----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client.get_default_database()\n",
    "products_col = db[PRODUCTS_COLL]\n",
    "signatures_col = db[SIG_COLL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "def get_char_shingles(text, k):\n",
    "    if not text:\n",
    "        return []\n",
    "    clean = \" \".join(str(text).lower().split())\n",
    "    if len(clean) < k:\n",
    "        return []\n",
    "    return [clean[i:i+k] for i in range(len(clean) - k + 1)]\n",
    "\n",
    "def compute_minhash(text, k, num_perm=MAX_HASHES):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    shingles = get_char_shingles(text, k)\n",
    "    for s in shingles:\n",
    "        m.update(s.encode(\"utf8\"))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def _worker_precompute(product):\n",
    "    asin = product.get(\"asin\")\n",
    "    title = product.get(\"title\") or \"\"\n",
    "    desc_field = product.get(\"description\") or \"\"\n",
    "    desc = \" \".join(desc_field) if isinstance(desc_field, list) else desc_field\n",
    "    hybrid = (title + \" \" + desc).strip()\n",
    "\n",
    "    out = {\"asin\": asin}\n",
    "    for k in K_VALUES:\n",
    "        out[f\"pst_k{k}\"] = compute_minhash(title, k)\n",
    "        out[f\"psd_k{k}\"] = compute_minhash(desc, k)\n",
    "        out[f\"pstd_k{k}\"] = compute_minhash(hybrid, k)\n",
    "    return out\n",
    "\n",
    "def precompute_all(products):\n",
    "    print(f\"âš¡ Precomputing MinHash objects ({NUM_WORKERS} workers)...\")\n",
    "    results = []\n",
    "    with Pool(processes=NUM_WORKERS) as pool:\n",
    "        for r in tqdm(pool.imap_unordered(_worker_precompute, products),\n",
    "                      total=len(products), file=sys.stdout):\n",
    "            results.append(r)\n",
    "\n",
    "    all_sigs = {}\n",
    "    for r in results:\n",
    "        asin = r[\"asin\"]\n",
    "        all_sigs[asin] = {k: {\"pst\": r[f\"pst_k{k}\"], \n",
    "                              \"psd\": r[f\"psd_k{k}\"], \n",
    "                              \"pstd\": r[f\"pstd_k{k}\"]} for k in K_VALUES}\n",
    "    print(\"âœ… Precompute finished.\")\n",
    "    return all_sigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5\n",
    "# def build_lsh_cached(all_sigs, k, num_hashes, num_bands):\n",
    "#     \"\"\"\n",
    "#     Build separate LSHs for pst, psd, pstd using precomputed MinHash objects.\n",
    "#     Returns (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash dict\n",
    "#     \"\"\"\n",
    "#     r = max(1, num_hashes // num_bands)\n",
    "#     lsh_pst = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "#     lsh_psd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "#     lsh_pstd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "#     asin_to_minhash = {\"pst\": {}, \"psd\": {}, \"pstd\": {}}\n",
    "\n",
    "#     for asin, sigs in all_sigs.items():\n",
    "#         # pst\n",
    "#         m_pst = MinHash(num_perm=num_hashes)\n",
    "#         m_pst.hashvalues = sigs[k][\"pst\"].hashvalues[:num_hashes]\n",
    "#         lsh_pst.insert(asin, m_pst)\n",
    "#         asin_to_minhash[\"pst\"][asin] = m_pst\n",
    "#         # psd\n",
    "#         m_psd = MinHash(num_perm=num_hashes)\n",
    "#         m_psd.hashvalues = sigs[k][\"psd\"].hashvalues[:num_hashes]\n",
    "#         lsh_psd.insert(asin, m_psd)\n",
    "#         asin_to_minhash[\"psd\"][asin] = m_psd\n",
    "#         # pstd\n",
    "#         m_pstd = MinHash(num_perm=num_hashes)\n",
    "#         m_pstd.hashvalues = sigs[k][\"pstd\"].hashvalues[:num_hashes]\n",
    "#         lsh_pstd.insert(asin, m_pstd)\n",
    "#         asin_to_minhash[\"pstd\"][asin] = m_pstd\n",
    "\n",
    "#     return (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash\n",
    "\n",
    "def build_lsh_all(all_sigs, k, num_hashes, num_bands):\n",
    "    r = max(1, num_hashes // num_bands)\n",
    "    \n",
    "    lsh_pst = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    lsh_psd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    lsh_pstd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    \n",
    "    asin_to_minhash = {\"pst\": {}, \"psd\": {}, \"pstd\": {}}\n",
    "    \n",
    "    for asin, sigs in all_sigs.items():\n",
    "        # Slice MinHash to match num_hashes\n",
    "        m_pst = MinHash(num_perm=num_hashes)\n",
    "        m_pst.hashvalues = sigs[k][\"pst\"].hashvalues[:num_hashes]\n",
    "        lsh_pst.insert(asin, m_pst)\n",
    "        asin_to_minhash[\"pst\"][asin] = m_pst\n",
    "        \n",
    "        m_psd = MinHash(num_perm=num_hashes)\n",
    "        m_psd.hashvalues = sigs[k][\"psd\"].hashvalues[:num_hashes]\n",
    "        lsh_psd.insert(asin, m_psd)\n",
    "        asin_to_minhash[\"psd\"][asin] = m_psd\n",
    "        \n",
    "        m_pstd = MinHash(num_perm=num_hashes)\n",
    "        m_pstd.hashvalues = sigs[k][\"pstd\"].hashvalues[:num_hashes]\n",
    "        lsh_pstd.insert(asin, m_pstd)\n",
    "        asin_to_minhash[\"pstd\"][asin] = m_pstd\n",
    "        \n",
    "    return (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff197570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def get_topk_similars(asin, m, lsh, asin_to_minhash, top_k=TOP_K):\n",
    "    candidates = lsh.query(m)\n",
    "    sims = []\n",
    "    for cand in candidates:\n",
    "        if cand == asin:\n",
    "            continue\n",
    "        score = m.jaccard(asin_to_minhash[cand])\n",
    "        sims.append((score, cand))\n",
    "    sims.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [{\"asin\": c, \"score\": float(s)} for s, c in sims[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907793ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7\n",
    "# def final_compute_and_write(products, all_sigs, best, signatures_col, top_k=TOP_K, bulk_batch=BULK_BATCH):\n",
    "#     print(\"\\nðŸš€ Computing top-10 similars with LSH...\")\n",
    "#     k = best[\"k\"]\n",
    "#     num_hashes = best[\"num_hashes\"]\n",
    "#     num_bands = best[\"num_bands\"]\n",
    "\n",
    "#     (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash = build_lsh_cached(all_sigs, k, num_hashes, num_bands)\n",
    "#     updates = []\n",
    "\n",
    "#     for p in tqdm(products, total=len(products), file=sys.stdout):\n",
    "#         asin = p[\"asin\"]\n",
    "#         sigs = all_sigs[asin][k]\n",
    "#         similars = {\n",
    "#             \"pst\": get_topk_similars(asin, sigs[\"pst\"], lsh_pst, asin_to_minhash[\"pst\"], top_k),\n",
    "#             \"psd\": get_topk_similars(asin, sigs[\"psd\"], lsh_psd, asin_to_minhash[\"psd\"], top_k),\n",
    "#             \"pstd\": get_topk_similars(asin, sigs[\"pstd\"], lsh_pstd, asin_to_minhash[\"pstd\"], top_k),\n",
    "#         }\n",
    "#         updates.append(UpdateOne(\n",
    "#             {\"asin\": asin},\n",
    "#             {\"$set\": {\n",
    "#                 \"asin\": asin,\n",
    "#                 \"pst_sig\": sigs[\"pst\"].hashvalues[:num_hashes].tolist(),\n",
    "#                 \"psd_sig\": sigs[\"psd\"].hashvalues[:num_hashes].tolist(),\n",
    "#                 \"pstd_sig\": sigs[\"pstd\"].hashvalues[:num_hashes].tolist(),\n",
    "#                 \"similar\": similars\n",
    "#             }},\n",
    "#             upsert=True\n",
    "#         ))\n",
    "#         if len(updates) >= bulk_batch:\n",
    "#             signatures_col.bulk_write(updates)\n",
    "#             updates = []\n",
    "\n",
    "#     if updates:\n",
    "#         signatures_col.bulk_write(updates)\n",
    "#     print(\"âœ… All results saved to DB.\")\n",
    "\n",
    "def final_compute_and_write(products, all_sigs, best, signatures_col, top_k=TOP_K, bulk_batch=BULK_BATCH):\n",
    "    print(\"\\nðŸš€ Computing top-k similars with LSH...\")\n",
    "    \n",
    "    k = best[\"k\"]\n",
    "    num_hashes = best[\"num_hashes\"]\n",
    "    num_bands = best[\"num_bands\"]\n",
    "    \n",
    "    (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash = build_lsh_all(all_sigs, k, num_hashes, num_bands)\n",
    "    \n",
    "    updates = []\n",
    "    for p in tqdm(products, total=len(products), file=sys.stdout):\n",
    "        asin = p[\"asin\"]\n",
    "        sigs = all_sigs[asin][k]\n",
    "        \n",
    "        # Slice MinHash objects before querying\n",
    "        m_pst = MinHash(num_perm=num_hashes)\n",
    "        m_pst.hashvalues = sigs[\"pst\"].hashvalues[:num_hashes]\n",
    "        \n",
    "        m_psd = MinHash(num_perm=num_hashes)\n",
    "        m_psd.hashvalues = sigs[\"psd\"].hashvalues[:num_hashes]\n",
    "        \n",
    "        m_pstd = MinHash(num_perm=num_hashes)\n",
    "        m_pstd.hashvalues = sigs[\"pstd\"].hashvalues[:num_hashes]\n",
    "        \n",
    "        similars = {\n",
    "            \"pst\": get_topk_similars(asin, m_pst, lsh_pst, asin_to_minhash[\"pst\"], top_k),\n",
    "            \"psd\": get_topk_similars(asin, m_psd, lsh_psd, asin_to_minhash[\"psd\"], top_k),\n",
    "            \"pstd\": get_topk_similars(asin, m_pstd, lsh_pstd, asin_to_minhash[\"pstd\"], top_k),\n",
    "        }\n",
    "        \n",
    "        updates.append(UpdateOne(\n",
    "            {\"asin\": asin},\n",
    "            {\"$set\": {\n",
    "                \"asin\": asin,\n",
    "                \"pst_sig\": sigs[\"pst\"].hashvalues[:num_hashes].tolist(),\n",
    "                \"psd_sig\": sigs[\"psd\"].hashvalues[:num_hashes].tolist(),\n",
    "                \"pstd_sig\": sigs[\"pstd\"].hashvalues[:num_hashes].tolist(),\n",
    "                \"similar\": similars\n",
    "            }},\n",
    "            upsert=True\n",
    "        ))\n",
    "        \n",
    "        if len(updates) >= bulk_batch:\n",
    "            signatures_col.bulk_write(updates)\n",
    "            updates = []\n",
    "    \n",
    "    if updates:\n",
    "        signatures_col.bulk_write(updates)\n",
    "    \n",
    "    print(\"âœ… All results saved to DB.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8\n",
    "# def tune_best_model(products, all_sigs, top_k=TOP_K, sample_size=TUNING_SAMPLE):\n",
    "#     print(\"ðŸ“Š Tuning best (k, num_hashes, num_bands) based on Precision@10...\")\n",
    "#     sampled = random.sample(products, sample_size) if sample_size < len(products) else products\n",
    "#     best_models = {\"pst\": None, \"psd\": None, \"pstd\": None}\n",
    "\n",
    "#     for key in [\"pst\", \"psd\", \"pstd\"]:\n",
    "#         best_score = -1\n",
    "#         for k in K_VALUES:\n",
    "#             for num_hashes in HASH_VALUES:\n",
    "#                 for num_bands in NUM_BANDS_VALUES:\n",
    "#                     r = max(1, num_hashes // num_bands)\n",
    "#                     # Build LSH only once per combination\n",
    "#                     lsh = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "#                     asin_to_mh = {}\n",
    "#                     for p in sampled:\n",
    "#                         asin = p[\"asin\"]\n",
    "#                         m = all_sigs[asin][k][key]\n",
    "#                         m_sliced = MinHash(num_perm=num_hashes)\n",
    "#                         m_sliced.hashvalues = m.hashvalues[:num_hashes]\n",
    "#                         lsh.insert(asin, m_sliced)\n",
    "#                         asin_to_mh[asin] = m_sliced\n",
    "#                     # Evaluate Precision@10\n",
    "#                     total_prec = 0\n",
    "#                     for p in sampled:\n",
    "#                         asin = p[\"asin\"]\n",
    "#                         topk = get_topk_similars(asin, asin_to_mh[asin], lsh, asin_to_mh, top_k)\n",
    "#                         # Check overlap with similar_asins from DB\n",
    "#                         actual = set(p.get(\"similar_asins\", []))\n",
    "#                         pred = set([x[\"asin\"] for x in topk])\n",
    "#                         # if actual:\n",
    "#                         #     total_prec += len(pred & actual) / min(len(pred), len(actual))\n",
    "#                         if actual and pred:\n",
    "#                             total_prec += len(pred & actual) / min(len(pred), len(actual))\n",
    "#                     avg_prec = total_prec / len([p for p in sampled if p.get(\"similar_asins\")])\n",
    "#                     if avg_prec > best_score:\n",
    "#                         best_score = avg_prec\n",
    "#                         best_models[key] = {\"k\": k, \"num_hashes\": num_hashes, \"num_bands\": num_bands, \"r\": r, \"precision\": avg_prec}\n",
    "#         print(f\"âœ… Best model for {key}: {best_models[key]}\")\n",
    "#     return best_models\n",
    "\n",
    "def tune_best_model(products, all_sigs, top_k=TOP_K, sample_size=TUNING_SAMPLE):\n",
    "    print(\"ðŸ“Š Tuning best (k, num_hashes, num_bands) based on Precision@10...\")\n",
    "    sampled = products\n",
    "    best_models = {\"pst\": None, \"psd\": None, \"pstd\": None}\n",
    "\n",
    "    for key in [\"pst\", \"psd\", \"pstd\"]:\n",
    "        best_score = -1\n",
    "        for k in K_VALUES:\n",
    "            for num_hashes in HASH_VALUES:\n",
    "                for num_bands in NUM_BANDS_VALUES:\n",
    "                    r = max(1, num_hashes // num_bands)\n",
    "\n",
    "                    # Skip invalid LSH params\n",
    "                    if num_bands * r > num_hashes:\n",
    "                        continue\n",
    "\n",
    "                    # Build LSH\n",
    "                    lsh = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "                    asin_to_mh = {}\n",
    "                    for p in sampled:\n",
    "                        asin = p[\"asin\"]\n",
    "                        m = all_sigs[asin][k][key]\n",
    "                        m_sliced = MinHash(num_perm=num_hashes)\n",
    "                        m_sliced.hashvalues = m.hashvalues[:num_hashes]\n",
    "                        lsh.insert(asin, m_sliced)\n",
    "                        asin_to_mh[asin] = m_sliced\n",
    "\n",
    "                    # Evaluate Precision@10\n",
    "                    total_prec = 0\n",
    "                    valid_count = 0\n",
    "                    for p in sampled:\n",
    "                        asin = p[\"asin\"]\n",
    "                        topk = get_topk_similars(asin, asin_to_mh[asin], lsh, asin_to_mh, top_k)\n",
    "                        actual = set(p.get(\"similar_asins\", []))\n",
    "                        pred = set([x[\"asin\"] for x in topk])\n",
    "                        if actual and pred:\n",
    "                            total_prec += len(pred & actual) / min(len(pred), len(actual))\n",
    "                            valid_count += 1\n",
    "\n",
    "                    if valid_count == 0:\n",
    "                        continue\n",
    "\n",
    "                    avg_prec = total_prec / valid_count\n",
    "                    if avg_prec > best_score:\n",
    "                        best_score = avg_prec\n",
    "                        best_models[key] = {\"k\": k, \"num_hashes\": num_hashes, \"num_bands\": num_bands, \"r\": r, \"precision\": avg_prec}\n",
    "\n",
    "        print(f\"âœ… Best model for {key}: {best_models[key]}\")\n",
    "    return best_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "products = list(products_col.find({}, {\"asin\":1, \"title\":1, \"description\":1, \"similar_asins\":1}))\n",
    "print(f\"ðŸ“¦ Found {len(products)} products.\")\n",
    "\n",
    "if products:\n",
    "    all_sigs = precompute_all(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc851fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if products:\n",
    "    best_models = tune_best_model(products, all_sigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b03225",
   "metadata": {},
   "outputs": [],
   "source": [
    "if products:\n",
    "    for key in [\"pst\", \"psd\", \"pstd\"]:\n",
    "        print(f\"\\nProcessing top-{TOP_K} for {key} using best model...\")\n",
    "        final_compute_and_write(products, all_sigs, best_models[key], signatures_col, top_k=TOP_K)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
