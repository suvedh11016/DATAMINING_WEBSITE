{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22929de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Jupyter Notebook version of full_pipeline_lsh.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017/cs5600\")\n",
    "PRODUCTS_COLL = os.getenv(\"PRODUCTS_COLL\", \"products\")\n",
    "SIG_COLL = os.getenv(\"SIG_COLL\", \"productsignatures\")\n",
    "\n",
    "K_VALUES = [2, 3, 5, 7, 10]\n",
    "HASH_VALUES = [10, 20, 50, 100, 150]\n",
    "NUM_BANDS_VALUES = [4, 5, 10, 15, 20, 25]\n",
    "MAX_HASHES = max(HASH_VALUES)\n",
    "NUM_WORKERS = max(1, min(cpu_count() - 1, 6))  # leave one core free, cap at 12\n",
    "TUNING_SAMPLE = 1000\n",
    "TOP_K = 10\n",
    "BULK_BATCH = 500\n",
    "# ----------------------------------------\n",
    "\n",
    "# ---------------- DB ----------------\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client.get_default_database()\n",
    "products_col = db[PRODUCTS_COLL]\n",
    "signatures_col = db[SIG_COLL]\n",
    "# ------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eccaf1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_shingles(text, k):\n",
    "    if not text:\n",
    "        return []\n",
    "    clean = \" \".join(str(text).lower().split())\n",
    "    if len(clean) < k:\n",
    "        return []\n",
    "    return [clean[i:i+k] for i in range(len(clean) - k + 1)]\n",
    "\n",
    "\n",
    "def compute_minhash(text, k, num_perm=MAX_HASHES):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    shingles = get_char_shingles(text, k)\n",
    "    for s in shingles:\n",
    "        m.update(s.encode(\"utf8\"))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _worker_precompute(product):\n",
    "    asin = product.get(\"asin\")\n",
    "    title = product.get(\"title\") or \"\"\n",
    "    desc_field = product.get(\"description\") or \"\"\n",
    "    desc = \" \".join(desc_field) if isinstance(desc_field, list) else desc_field\n",
    "    hybrid = (title + \" \" + desc).strip()\n",
    "\n",
    "    out = {\"asin\": asin}\n",
    "    for k in K_VALUES:\n",
    "        out[f\"pst_k{k}\"] = compute_minhash(title, k)\n",
    "        out[f\"psd_k{k}\"] = compute_minhash(desc, k)\n",
    "        out[f\"pstd_k{k}\"] = compute_minhash(hybrid, k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def precompute_all(products):\n",
    "    print(f\"âš¡ Precomputing MinHash objects ({NUM_WORKERS} workers)...\")\n",
    "    results = []\n",
    "    with Pool(processes=NUM_WORKERS) as pool:\n",
    "        for r in tqdm(pool.imap_unordered(_worker_precompute, products),\n",
    "                      total=len(products), file=sys.stdout):\n",
    "            results.append(r)\n",
    "\n",
    "    all_sigs = {}\n",
    "    for r in results:\n",
    "        asin = r[\"asin\"]\n",
    "        all_sigs[asin] = {k: {\"pst\": r[f\"pst_k{k}\"], \n",
    "                              \"psd\": r[f\"psd_k{k}\"], \n",
    "                              \"pstd\": r[f\"pstd_k{k}\"]} for k in K_VALUES}\n",
    "    print(\"âœ… Precompute finished.\")\n",
    "    return all_sigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6584dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bands_from_signature(sig, num_hashes, num_bands):\n",
    "    hv = sig.hashvalues[:num_hashes]\n",
    "    rows_per_band = max(1, num_hashes // num_bands)\n",
    "    bands = []\n",
    "    for b in range(num_bands):\n",
    "        start = b * rows_per_band\n",
    "        band = tuple(hv[start:start+rows_per_band])\n",
    "        if band:\n",
    "            bands.append(band)\n",
    "    return bands\n",
    "\n",
    "\n",
    "def candidate_probability(s, r, b):\n",
    "    return 1.0 - (1.0 - (s ** r)) ** b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5545bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_parameters(products, all_sigs, sample_size=TUNING_SAMPLE):\n",
    "    print(\"ðŸ“Š Tuning parameters (k, num_hashes, num_bands)...\")\n",
    "    sampled = random.sample(products, sample_size) if sample_size and sample_size < len(products) else products\n",
    "    best = None\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        for num_hashes in HASH_VALUES:\n",
    "            for num_bands in NUM_BANDS_VALUES:\n",
    "                r = max(1, num_hashes // num_bands)\n",
    "                seen = set()\n",
    "                overlaps = 0\n",
    "\n",
    "                for p in sampled:\n",
    "                    asin = p[\"asin\"]\n",
    "                    sigs = all_sigs.get(asin)\n",
    "                    if not sigs:\n",
    "                        continue\n",
    "                    sig = sigs[k][\"pstd\"]\n",
    "                    bands = compute_bands_from_signature(sig, num_hashes, num_bands)\n",
    "                    if any(b in seen for b in bands):\n",
    "                        overlaps += 1\n",
    "                    for b in bands:\n",
    "                        seen.add(b)\n",
    "\n",
    "                recall = overlaps / max(1, len(sampled))\n",
    "                pLow = candidate_probability(0.4, r, num_bands)\n",
    "                pHigh = candidate_probability(0.8, r, num_bands)\n",
    "                theoryScore = pHigh - pLow\n",
    "                penalty = 0.5 if (recall < 0.3 or recall > 0.9) else 0.0\n",
    "                score = theoryScore - penalty\n",
    "\n",
    "                if not best or score > best[\"score\"]:\n",
    "                    best = {\"k\": k, \"num_hashes\": num_hashes, \"num_bands\": num_bands, \"r\": r,\n",
    "                            \"overlaps\": overlaps, \"recall\": recall, \"pLow\": pLow, \"pHigh\": pHigh, \"score\": score}\n",
    "    print(\"âœ… Best configuration:\", best)\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f42c83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_lsh(all_sigs, k, num_hashes, num_bands):\n",
    "#     r = max(1, num_hashes // num_bands)\n",
    "#     lsh = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "#     asin_to_minhash = {}\n",
    "\n",
    "#     for asin, sigs in all_sigs.items():\n",
    "#         m = MinHash(num_perm=num_hashes)\n",
    "#         m.hashvalues = sigs[k][\"pstd\"].hashvalues[:num_hashes]\n",
    "#         lsh.insert(asin, m)\n",
    "#         asin_to_minhash[asin] = m\n",
    "#     return lsh, asin_to_minhash\n",
    "\n",
    "\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from pymongo import UpdateOne\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def build_lsh_all(all_sigs, k, num_hashes, num_bands):\n",
    "    \"\"\"\n",
    "    Build three separate LSH indexes (pst, psd, pstd) and return mapping\n",
    "    asin_to_minhash for each type.\n",
    "    \"\"\"\n",
    "    r = max(1, num_hashes // num_bands)\n",
    "    lsh_pst = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    lsh_psd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    lsh_pstd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "\n",
    "    asin_to_minhash = {\"pst\": {}, \"psd\": {}, \"pstd\": {}}\n",
    "\n",
    "    for asin, sigs in all_sigs.items():\n",
    "        # pst\n",
    "        m_pst = MinHash(num_perm=num_hashes)\n",
    "        m_pst.hashvalues = sigs[k][\"pst\"].hashvalues[:num_hashes]\n",
    "        lsh_pst.insert(asin, m_pst)\n",
    "        asin_to_minhash[\"pst\"][asin] = m_pst\n",
    "\n",
    "        # psd\n",
    "        m_psd = MinHash(num_perm=num_hashes)\n",
    "        m_psd.hashvalues = sigs[k][\"psd\"].hashvalues[:num_hashes]\n",
    "        lsh_psd.insert(asin, m_psd)\n",
    "        asin_to_minhash[\"psd\"][asin] = m_psd\n",
    "\n",
    "        # pstd\n",
    "        m_pstd = MinHash(num_perm=num_hashes)\n",
    "        m_pstd.hashvalues = sigs[k][\"pstd\"].hashvalues[:num_hashes]\n",
    "        lsh_pstd.insert(asin, m_pstd)\n",
    "        asin_to_minhash[\"pstd\"][asin] = m_pstd\n",
    "\n",
    "    return (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash\n",
    "\n",
    "\n",
    "def get_topk_similars(asin, m, lsh, asin_to_minhash, top_k=10):\n",
    "    \"\"\"\n",
    "    Query LSH for candidates, compute Jaccard similarity, return top-K.\n",
    "    \"\"\"\n",
    "    candidates = lsh.query(m)\n",
    "    sims = []\n",
    "    for cand in candidates:\n",
    "        if cand == asin:\n",
    "            continue\n",
    "        score = m.jaccard(asin_to_minhash[cand])\n",
    "        sims.append((score, cand))\n",
    "    sims.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [{\"asin\": c, \"score\": float(s)} for s, c in sims[:top_k]]\n",
    "\n",
    "\n",
    "def final_compute_and_write(products, all_sigs, best, signatures_col, top_k=10, bulk_batch=500):\n",
    "    print(\"\\nðŸš€ Computing top-10 similars with LSH...\")\n",
    "    k = best[\"k\"]\n",
    "    num_hashes = best[\"num_hashes\"]\n",
    "    num_bands = best[\"num_bands\"]\n",
    "\n",
    "    (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash = build_lsh_all(all_sigs, k, num_hashes, num_bands)\n",
    "    updates = []\n",
    "\n",
    "    for p in tqdm(products, total=len(products), file=sys.stdout):\n",
    "        asin = p[\"asin\"]\n",
    "        sigs = all_sigs[asin][k]\n",
    "\n",
    "        similars = {\n",
    "            \"pst\": get_topk_similars(asin, sigs[\"pst\"], lsh_pst, asin_to_minhash[\"pst\"], top_k),\n",
    "            \"psd\": get_topk_similars(asin, sigs[\"psd\"], lsh_psd, asin_to_minhash[\"psd\"], top_k),\n",
    "            \"pstd\": get_topk_similars(asin, sigs[\"pstd\"], lsh_pstd, asin_to_minhash[\"pstd\"], top_k),\n",
    "        }\n",
    "\n",
    "        updates.append(UpdateOne(\n",
    "            {\"asin\": asin},\n",
    "            {\"$set\": {\n",
    "                \"asin\": asin,\n",
    "                \"pst_sig\": sigs[\"pst\"].hashvalues[:num_hashes].tolist(),\n",
    "                \"psd_sig\": sigs[\"psd\"].hashvalues[:num_hashes].tolist(),\n",
    "                \"pstd_sig\": sigs[\"pstd\"].hashvalues[:num_hashes].tolist(),\n",
    "                \"similar\": similars\n",
    "            }},\n",
    "            upsert=True\n",
    "        ))\n",
    "\n",
    "        if len(updates) >= bulk_batch:\n",
    "            signatures_col.bulk_write(updates)\n",
    "            updates = []\n",
    "\n",
    "    if updates:\n",
    "        signatures_col.bulk_write(updates)\n",
    "    print(\"âœ… All results saved to DB.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def final_compute_and_write(products, all_sigs, best):\n",
    "#     print(\"\\nðŸš€ Computing top-10 similars with LSH...\")\n",
    "#     k = best[\"k\"]\n",
    "#     num_hashes = best[\"num_hashes\"]\n",
    "#     num_bands = best[\"num_bands\"]\n",
    "\n",
    "#     (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash = build_lsh_all(all_sigs, k, num_hashes, num_bands)\n",
    "#     updates = []\n",
    "\n",
    "#     for p in tqdm(products, total=len(products), file=sys.stdout):\n",
    "#         asin = p[\"asin\"]\n",
    "#         sigs = all_sigs[asin][k]\n",
    "\n",
    "#         # Wrap hash lists into MinHash objects for querying\n",
    "#         m_pst = MinHash(num_perm=num_hashes)\n",
    "#         m_pst.hashvalues = np.array(sigs[\"pst\"][:num_hashes], dtype='uint64')\n",
    "\n",
    "#         m_psd = MinHash(num_perm=num_hashes)\n",
    "#         m_psd.hashvalues = np.array(sigs[\"psd\"][:num_hashes], dtype='uint64')\n",
    "\n",
    "#         m_pstd = MinHash(num_perm=num_hashes)\n",
    "#         m_pstd.hashvalues = np.array(sigs[\"pstd\"][:num_hashes], dtype='uint64')\n",
    "\n",
    "#         similars = {\n",
    "#             \"pst\": get_topk_similars(asin, m_pst, lsh_pst, asin_to_minhash[\"pst\"]),\n",
    "#             \"psd\": get_topk_similars(asin, m_psd, lsh_psd, asin_to_minhash[\"psd\"]),\n",
    "#             \"pstd\": get_topk_similars(asin, m_pstd, lsh_pstd, asin_to_minhash[\"pstd\"]),\n",
    "#         }\n",
    "\n",
    "#         updates.append(UpdateOne(\n",
    "#             {\"asin\": asin},\n",
    "#             {\"$set\": {\n",
    "#                 \"asin\": asin,\n",
    "#                 \"pst_sig\": sigs[\"pst\"][:num_hashes],\n",
    "#                 \"psd_sig\": sigs[\"psd\"][:num_hashes],\n",
    "#                 \"pstd_sig\": sigs[\"pstd\"][:num_hashes],\n",
    "#                 \"similar\": similars\n",
    "#             }},\n",
    "#             upsert=True\n",
    "#         ))\n",
    "\n",
    "#         if len(updates) >= BULK_BATCH:\n",
    "#             signatures_col.bulk_write(updates)\n",
    "#             updates = []\n",
    "\n",
    "#     if updates:\n",
    "#         signatures_col.bulk_write(updates)\n",
    "\n",
    "#     print(\"âœ… All results saved to DB.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f4820a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# products = list(products_col.find({}, {\"asin\":1, \"title\":1, \"description\":1}))\n",
    "# print(f\"ðŸ“¦ Found {len(products)} products.\")\n",
    "# if not products:\n",
    "#     print(\"No products in DB â€” exiting.\")\n",
    "# else:\n",
    "#     all_sigs = precompute_all(products)\n",
    "#     best = tune_parameters(products, all_sigs, sample_size=TUNING_SAMPLE)\n",
    "#     final_compute_and_write(products, all_sigs, best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e41279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Found 30239 products.\n"
     ]
    }
   ],
   "source": [
    "products = list(products_col.find({}, {\"asin\":1, \"title\":1, \"description\":1}))\n",
    "print(f\"ðŸ“¦ Found {len(products)} products.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce9bb68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Precomputing MinHash objects (6 workers)...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30239/30239 [04:11<00:00, 120.14it/s]\n",
      "âœ… Precompute finished.\n"
     ]
    }
   ],
   "source": [
    "if products:\n",
    "    all_sigs = precompute_all(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "301bbcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Tuning parameters (k, num_hashes, num_bands)...\n",
      "âœ… Best configuration: {'k': 2, 'num_hashes': 150, 'num_bands': 20, 'r': 7, 'overlaps': 595, 'recall': 0.595, 'pLow': 0.032262951677010765, 'pHigh': 0.9909703154270351, 'score': 0.9587073637500243}\n"
     ]
    }
   ],
   "source": [
    "if products and all_sigs:\n",
    "    best = tune_parameters(products, all_sigs, sample_size=TUNING_SAMPLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d180b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Computing top-10 similars with LSH...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30239/30239 [04:30<00:00, 111.96it/s]\n",
      "âœ… All results saved to DB.\n"
     ]
    }
   ],
   "source": [
    "if products and all_sigs and best:\n",
    "    final_compute_and_write(products, all_sigs, best, signatures_col)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
