{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c130e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Found 30239 products.\n"
     ]
    }
   ],
   "source": [
    "# ================= Cell 1: Imports & Config =================\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017/cs5600\")\n",
    "PRODUCTS_COLL = os.getenv(\"PRODUCTS_COLL\", \"products\")\n",
    "SIG_COLL = os.getenv(\"SIG_COLL\", \"productsignatures\")\n",
    "\n",
    "K_VALUES = [2, 3, 5, 7, 10]\n",
    "# HASH_VALUES = [10, 20, 30, 50, 100, 150]\n",
    "HASH_VALUES = [150]\n",
    "NUM_BANDS_VALUES = [4, 5, 10, 15, 20]\n",
    "NUM_WORKERS = max(1, min(cpu_count() - 1, 12))\n",
    "TOP_K = 10\n",
    "BULK_BATCH = 500\n",
    "TUNING_SAMPLE = 100  # Top-100 products with most similar_asins\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ================= Cell 2: MongoDB =================\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client.get_default_database()\n",
    "products_col = db[PRODUCTS_COLL]\n",
    "signatures_col = db[SIG_COLL]\n",
    "\n",
    "# ================= Cell 3: Word Shingles & MinHash =================\n",
    "def get_word_shingles(text, k):\n",
    "    if not text:\n",
    "        return []\n",
    "    words = str(text).lower().split()\n",
    "    if len(words) < k:\n",
    "        return [\" \".join(words)] if words else [\"dummy\"]\n",
    "    return [\" \".join(words[i:i+k]) for i in range(len(words) - k + 1)]\n",
    "\n",
    "def compute_minhash_words(text, k, num_perm=150):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    shingles = get_word_shingles(text, k)\n",
    "    for s in shingles:\n",
    "        m.update(s.encode(\"utf8\"))\n",
    "    return m\n",
    "\n",
    "# ================= Cell 4: Precompute MinHash =================\n",
    "def _worker_precompute_words(product):\n",
    "    asin = product.get(\"asin\")\n",
    "    title = product.get(\"title\") or \"\"\n",
    "    desc_field = product.get(\"description\") or \"\"\n",
    "    desc = \" \".join(desc_field) if isinstance(desc_field, list) else desc_field\n",
    "    hybrid = (title + \" \" + desc).strip()\n",
    "\n",
    "    out = {\"asin\": asin}\n",
    "    for k in K_VALUES:\n",
    "        out[f\"pst_k{k}\"] = compute_minhash_words(title, k)\n",
    "        out[f\"psd_k{k}\"] = compute_minhash_words(desc, k)\n",
    "        out[f\"pstd_k{k}\"] = compute_minhash_words(hybrid, k)\n",
    "    return out\n",
    "\n",
    "def precompute_all(products):\n",
    "    print(f\"âš¡ Precomputing MinHash objects ({NUM_WORKERS} workers)...\")\n",
    "    results = []\n",
    "    with Pool(processes=NUM_WORKERS) as pool:\n",
    "        for r in tqdm(pool.imap_unordered(_worker_precompute_words, products),\n",
    "                      total=len(products), file=sys.stdout):\n",
    "            results.append(r)\n",
    "\n",
    "    all_sigs = {}\n",
    "    for r in results:\n",
    "        asin = r[\"asin\"]\n",
    "        all_sigs[asin] = {k: {\"pst\": r[f\"pst_k{k}\"], \n",
    "                              \"psd\": r[f\"psd_k{k}\"], \n",
    "                              \"pstd\": r[f\"pstd_k{k}\"]} for k in K_VALUES}\n",
    "    print(\"âœ… Precompute finished.\")\n",
    "    return all_sigs\n",
    "\n",
    "# ================= Cell 5: Build LSH =================\n",
    "def build_lsh_all(all_sigs, k, num_hashes, num_bands):\n",
    "    r = max(1, num_hashes // num_bands)\n",
    "    lsh_pst = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    lsh_psd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    lsh_pstd = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "    asin_to_minhash = {\"pst\": {}, \"psd\": {}, \"pstd\": {}}\n",
    "\n",
    "    for asin, sigs in all_sigs.items():\n",
    "        m_pst = MinHash(num_perm=num_hashes)\n",
    "        m_pst.hashvalues = sigs[k][\"pst\"].hashvalues[:num_hashes]\n",
    "        lsh_pst.insert(asin, m_pst)\n",
    "        asin_to_minhash[\"pst\"][asin] = m_pst\n",
    "\n",
    "        m_psd = MinHash(num_perm=num_hashes)\n",
    "        m_psd.hashvalues = sigs[k][\"psd\"].hashvalues[:num_hashes]\n",
    "        lsh_psd.insert(asin, m_psd)\n",
    "        asin_to_minhash[\"psd\"][asin] = m_psd\n",
    "\n",
    "        m_pstd = MinHash(num_perm=num_hashes)\n",
    "        m_pstd.hashvalues = sigs[k][\"pstd\"].hashvalues[:num_hashes]\n",
    "        lsh_pstd.insert(asin, m_pstd)\n",
    "        asin_to_minhash[\"pstd\"][asin] = m_pstd\n",
    "\n",
    "    return (lsh_pst, lsh_psd, lsh_pstd), asin_to_minhash\n",
    "\n",
    "# ================= Cell 6: Top-K Similar =================\n",
    "def get_topk_similars(asin, m, lsh, asin_to_minhash, top_k=TOP_K):\n",
    "    candidates = lsh.query(m)\n",
    "    sims = []\n",
    "    for cand in candidates:\n",
    "        if cand == asin:\n",
    "            continue\n",
    "        score = m.jaccard(asin_to_minhash[cand])\n",
    "        sims.append((score, cand))\n",
    "    sims.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [{\"asin\": c, \"score\": float(s)} for s, c in sims[:top_k]]\n",
    "\n",
    "# ================= Cell 7: Grid Search for Best Models =================\n",
    "def tune_best_model(products, all_sigs, top_k=TOP_K, sample_size=TUNING_SAMPLE):\n",
    "    print(\"ðŸ“Š Tuning best (k, num_hashes, num_bands) based on Precision@10...\")\n",
    "    # Pick top-100 products with most similar_asins\n",
    "    products = sorted([p for p in products if p.get(\"similar_asins\")], \n",
    "                      key=lambda x: len(x.get(\"similar_asins\", [])), reverse=True)[:sample_size]\n",
    "    best_models = {\"pst\": None, \"psd\": None, \"pstd\": None}\n",
    "\n",
    "    for key in [\"pst\", \"psd\", \"pstd\"]:\n",
    "        best_score = -1\n",
    "        for k in K_VALUES:\n",
    "            for num_hashes in HASH_VALUES:\n",
    "                for num_bands in NUM_BANDS_VALUES:\n",
    "                    r = max(1, num_hashes // num_bands)\n",
    "                    if num_bands * r > num_hashes:\n",
    "                        continue\n",
    "\n",
    "                    lsh = MinHashLSH(num_perm=num_hashes, params=(num_bands, r))\n",
    "                    asin_to_mh = {}\n",
    "                    for p in products:\n",
    "                        asin = p[\"asin\"]\n",
    "                        m = all_sigs[asin][k][key]\n",
    "                        m_sliced = MinHash(num_perm=num_hashes)\n",
    "                        m_sliced.hashvalues = m.hashvalues[:num_hashes].copy()\n",
    "\n",
    "                        lsh.insert(asin, m_sliced)\n",
    "                        asin_to_mh[asin] = m_sliced\n",
    "\n",
    "                    total_prec = 0\n",
    "                    valid_count = 0\n",
    "                    for p in products:\n",
    "                        asin = p[\"asin\"]\n",
    "                        topk = get_topk_similars(asin, asin_to_mh[asin], lsh, asin_to_mh, top_k)\n",
    "                        actual = set(p.get(\"similar_asins\", []))\n",
    "                        pred = set([x[\"asin\"] for x in topk])\n",
    "                        if actual and pred:\n",
    "                            total_prec += len(pred & actual) / min(len(pred), len(actual))\n",
    "                            valid_count += 1\n",
    "\n",
    "                    if valid_count == 0:\n",
    "                        continue\n",
    "\n",
    "                    avg_prec = total_prec / valid_count\n",
    "                    if avg_prec > best_score:\n",
    "                        best_score = avg_prec\n",
    "                        best_models[key] = {\"k\": k, \"num_hashes\": num_hashes, \"num_bands\": num_bands, \"r\": r, \"precision\": avg_prec}\n",
    "\n",
    "        print(f\"âœ… Best model for {key}: {best_models[key]}\")\n",
    "    return best_models\n",
    "\n",
    "# ================= Cell 8: Final Compute & Save =================\n",
    "def final_compute_and_write(products, all_sigs, best_models, signatures_col, top_k=TOP_K, bulk_batch=BULK_BATCH):\n",
    "    print(\"\\nðŸš€ Computing top-k similars with LSH using best models...\")\n",
    "\n",
    "    key_to_idx = {\"pst\": 0, \"psd\": 1, \"pstd\": 2}\n",
    "    updates = []\n",
    "\n",
    "    for key in [\"pst\", \"psd\", \"pstd\"]:\n",
    "        best = best_models[key]\n",
    "        k_best = best[\"k\"]\n",
    "        num_hashes = best[\"num_hashes\"]\n",
    "        num_bands = best[\"num_bands\"]\n",
    "\n",
    "        # Build LSH and get MinHash dict\n",
    "        lsh, asin_to_mh = build_lsh_all(all_sigs, k_best, num_hashes, num_bands)\n",
    "\n",
    "        for p in tqdm(products, total=len(products), file=sys.stdout):\n",
    "            asin = p[\"asin\"]\n",
    "            sigs = all_sigs[asin][k_best]\n",
    "\n",
    "            # Slice MinHash object\n",
    "            m = MinHash(num_perm=num_hashes)\n",
    "            m.hashvalues = sigs[key].hashvalues[:num_hashes]\n",
    "\n",
    "            # Query LSH\n",
    "            lsh_obj = lsh[key_to_idx[key]]\n",
    "            topk = get_topk_similars(asin, m, lsh_obj, asin_to_mh[key], top_k)\n",
    "\n",
    "            # Save only that subfield in MongoDB\n",
    "            updates.append(UpdateOne(\n",
    "                {\"asin\": asin},\n",
    "                {\"$set\": {\n",
    "                    \"asin\": asin,\n",
    "                    f\"{key}_sig\": sigs[key].hashvalues[:num_hashes].tolist(),\n",
    "                    f\"similar.{key}\": topk\n",
    "                }},\n",
    "                upsert=True\n",
    "            ))\n",
    "\n",
    "            if len(updates) >= bulk_batch:\n",
    "                signatures_col.bulk_write(updates)\n",
    "                updates = []\n",
    "\n",
    "    if updates:\n",
    "        signatures_col.bulk_write(updates)\n",
    "\n",
    "    print(\"âœ… All results saved to DB.\")\n",
    "\n",
    "# ================= Cell 9: Run Pipeline =================\n",
    "products = list(products_col.find({}, {\"asin\":1, \"title\":1, \"description\":1, \"similar_asins\":1}))\n",
    "print(f\"ðŸ“¦ Found {len(products)} products.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2431b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Precomputing MinHash objects (12 workers)...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30239/30239 [01:36<00:00, 311.85it/s]\n",
      "âœ… Precompute finished.\n"
     ]
    }
   ],
   "source": [
    "if products:\n",
    "    all_sigs = precompute_all(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82e32956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Tuning best (k, num_hashes, num_bands) based on Precision@10...\n",
      "âœ… Best model for pst: {'k': 2, 'num_hashes': 150, 'num_bands': 20, 'r': 7, 'precision': 0.16666666666666666}\n",
      "âœ… Best model for psd: {'k': 2, 'num_hashes': 150, 'num_bands': 15, 'r': 10, 'precision': 0.25}\n",
      "âœ… Best model for pstd: {'k': 2, 'num_hashes': 150, 'num_bands': 15, 'r': 10, 'precision': 0.375}\n"
     ]
    }
   ],
   "source": [
    "if products:\n",
    "    best_models = tune_best_model(products, all_sigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85d1a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Computing top-k similars with LSH using best models...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30239/30239 [00:35<00:00, 862.24it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30239/30239 [00:56<00:00, 530.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30239/30239 [00:39<00:00, 759.08it/s] \n",
      "âœ… All results saved to DB.\n"
     ]
    }
   ],
   "source": [
    "if products:\n",
    "    final_compute_and_write(products, all_sigs, best_models, signatures_col, top_k=TOP_K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
